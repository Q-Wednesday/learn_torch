{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitpytorchgpucondad001d335bb3e4cd7a60eb912d5940cc0",
   "display_name": "Python 3.7.3 64-bit ('pytorch_gpu': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[8.7245e-39, 9.2755e-39, 8.9082e-39],\n        [9.9184e-39, 8.4490e-39, 9.6429e-39],\n        [1.0653e-38, 1.0469e-38, 4.2246e-39],\n        [1.0378e-38, 9.6429e-39, 9.2755e-39],\n        [9.7346e-39, 1.0745e-38, 1.0102e-38]])\n"
    }
   ],
   "source": [
    "x=torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[0.5136, 0.3728, 0.0700],\n        [0.9629, 0.9671, 0.9213],\n        [0.7046, 0.6868, 0.1409],\n        [0.3271, 0.9426, 0.4022],\n        [0.9171, 0.9477, 0.8985]])\n"
    }
   ],
   "source": [
    "x=torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n"
    }
   ],
   "source": [
    "x=torch.zeros(5,3,dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([5.5000, 3.0000])\n"
    }
   ],
   "source": [
    "x=torch.tensor([5.5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\ntensor([[ 1.5580,  0.4026,  1.5441],\n        [ 1.1689, -0.1344,  0.1588],\n        [-0.1300, -1.2169, -0.2444],\n        [-1.6961, -1.3372,  0.7064],\n        [ 0.2113, -0.4972,  0.0542]])\n"
    }
   ],
   "source": [
    "x=x.new_ones(5,3,dtype=torch.float64)\n",
    "print(x)\n",
    "x=torch.randn_like(x,dtype=torch.float)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([5, 3])\ntorch.Size([5, 3])\n"
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)#返回值是tuple支持tuple的所有操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 2.3735,  0.6952,  1.6369],\n        [ 2.0509,  0.4700,  0.1632],\n        [ 0.3748, -0.2851, -0.1982],\n        [-0.9946, -1.3075,  1.1628],\n        [ 0.9147,  0.3067,  0.5907]])\n"
    }
   ],
   "source": [
    "y=torch.rand(5,3)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 2.3735,  0.6952,  1.6369],\n        [ 2.0509,  0.4700,  0.1632],\n        [ 0.3748, -0.2851, -0.1982],\n        [-0.9946, -1.3075,  1.1628],\n        [ 0.9147,  0.3067,  0.5907]])\ntensor([[ 2.3735,  0.6952,  1.6369],\n        [ 2.0509,  0.4700,  0.1632],\n        [ 0.3748, -0.2851, -0.1982],\n        [-0.9946, -1.3075,  1.1628],\n        [ 0.9147,  0.3067,  0.5907]])\n"
    }
   ],
   "source": [
    "print(torch.add(x,y))\n",
    "result=torch.empty(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.3735,  0.6952,  1.6369],\n        [ 2.0509,  0.4700,  0.1632],\n        [ 0.3748, -0.2851, -0.1982],\n        [-0.9946, -1.3075,  1.1628],\n        [ 0.9147,  0.3067,  0.5907]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)#所有inplace版本都有后缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([2.5580, 1.4026, 2.5441])\ntensor([2.5580, 1.4026, 2.5441])\n"
    }
   ],
   "source": [
    "#索引，类似numpy的索引操作来访问，索引出来的结果是和原数据共享内存的\n",
    "y=x[0,:]\n",
    "y+=1\n",
    "print(y)\n",
    "print(x[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([ 2.5580,  1.4026,  2.5441,  1.1689, -0.1344,  0.1588, -0.1300, -1.2169,\n        -0.2444, -1.6961, -1.3372,  0.7064,  0.2113, -0.4972,  0.0542])\ntensor([ 2.5580,  1.4026,  2.5441,  1.1689, -0.1344,  0.1588, -0.1300, -1.2169,\n        -0.2444, -1.6961, -1.3372,  0.7064,  0.2113, -0.4972,  0.0542])\n"
    }
   ],
   "source": [
    "#用view来改变tensor的形状，但是是共享data的\n",
    "y=x.view(15)\n",
    "print(y)\n",
    "#要获得真的副本，需要用reshape()改变形状,但是不能保证返回拷贝，所以推荐用clone创建一个副本再用view()\n",
    "x_cp=x.clone().view(15)\n",
    "print(x_cp)\n",
    "#而且使用clone会被记录在计算图中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([-1.8057])\n-1.8056740760803223\n"
    }
   ],
   "source": [
    "#item()把标量转化成python number\n",
    "x=torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False\n"
    }
   ],
   "source": [
    "#内存开销：索引操作不会开辟新内存，但是y=x+y会开新内存，然后y指向新的内存。python自带的id函数表示内存地址，用此实验\n",
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "y=y+x\n",
    "print(id(y)==id_before)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\n"
    }
   ],
   "source": [
    "#如果要把结果放在原来y的内存，则用索引进行操作\n",
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "y[:]=y+x\n",
    "print(id(y)==id_before)\n",
    "\n",
    "#虽然view返回的Tensor与原来的是共享data的，但是依然是一个新的Tensor,有不同的id,因为除了data外，tensor还包含其他的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\ntensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n"
    }
   ],
   "source": [
    "#tensor和Numpy相互转换，注意转换后是公用内存的。但是用torch.tensor()方法就会拷贝\n",
    "a=torch.ones(5)\n",
    "b=a.numpy()\n",
    "print(a,b)\n",
    "\n",
    "a+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.ones(5)\n",
    "b=torch.from_numpy(a)\n",
    "print(a,b)\n",
    "\n",
    "a+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[3. 3. 3. 3. 3.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "c=torch.tensor(a)\n",
    "a+=1\n",
    "print(a,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([2, 3], device='cuda:0')\ntensor([2., 3.], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "#tensor on GPU\n",
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda\")\n",
    "    y=torch.ones_like(x,device=device)\n",
    "    x=x.to(device)\n",
    "    z=x+y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\",torch.double))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "自动求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nNone\n"
    }
   ],
   "source": [
    "#Tensor是这个包的核心类，把.requires_grad设为true,就会开始追踪所有操作，完成计算后调用backward()就可以完成所有梯度计算，梯度累积到.grad属性中\n",
    "#不想被继续追踪就调用detach().或使用with torch.no_grad()\n",
    "#Function是一个很重要的类，与Tensor结合就能构建一个记录整个计算过程的有向无环图，每个Tensor都有.grad_fn属性，该属性是创建该Tensor的Function\n",
    "x=torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward0>)\n<AddBackward0 object at 0x0000024ABB99A048>\n"
    }
   ],
   "source": [
    "y=x+2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "#x是直接创建的，没有梯度，称为叶子节点，而y是通过加法创建的，有一个为AddBackward的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True False\n"
    }
   ],
   "source": [
    "print(x.is_leaf,y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
    }
   ],
   "source": [
    "z=y*y*3\n",
    "out=z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False\nTrue\n<SumBackward0 object at 0x0000024AB69C7C88>\n"
    }
   ],
   "source": [
    "a=torch.randn(2,2)#缺失情况下，默认requires_grad=False\n",
    "a=((a*3)/(a-1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b=(a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于outr是一个标量，所以调用backward()时不需要指定求导变量\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])\n"
    }
   ],
   "source": [
    "print(x.grad)#这是out关于x的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[5.5000, 5.5000],\n        [5.5000, 5.5000]])\ntensor([[1., 1.],\n        [1., 1.]])\n"
    }
   ],
   "source": [
    "out2=x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3=x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[2., 4.],\n        [6., 8.]], grad_fn=<ViewBackward>)\n"
    }
   ],
   "source": [
    "x=torch.tensor([1.0,2.0,3.0,4.0],requires_grad=True)\n",
    "y=2*x\n",
    "z=y.view(2,2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
    }
   ],
   "source": [
    "v=torch.tensor([[1.0,0.1],[0.01,0.001]],dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\ntensor(1., grad_fn=<PowBackward0>) True\ntensor(1.) False\ntensor(2., grad_fn=<AddBackward0>) True\n"
    }
   ],
   "source": [
    "#中断梯度追踪\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2 \n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # False\n",
    "print(y3, y3.requires_grad) # True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(2.)\n"
    }
   ],
   "source": [
    "y3.backward()\n",
    "print(x.grad)\n",
    "#y2的梯度是不能回传的，y2也不能调用backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([1.])\nFalse\ntensor([100.], requires_grad=True)\ntensor([2.])\n"
    }
   ],
   "source": [
    "#想更改tensor的数值又不被autograd记录，可以对tensor.data进行操作\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}