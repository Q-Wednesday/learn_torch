{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitpytorchgpucondad001d335bb3e4cd7a60eb912d5940cc0",
   "display_name": "Python 3.7.3 64-bit ('pytorch_gpu': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#线性回归简洁实现\n",
    "import torch\n",
    "import numpy as np\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch提供data包来读取数据\n",
    "import torch.utils.data as Data\n",
    "\n",
    "batch_size=10\n",
    "#将训练数据的特征和标签组合\n",
    "dataset=Data.TensorDataset(features,labels)\n",
    "#随机读取小批量\n",
    "data_iter=Data.DataLoader(dataset,batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[-0.6403, -0.3721],\n        [-0.1560, -0.5522],\n        [ 0.2344,  0.3521],\n        [ 0.3034, -0.7525],\n        [ 1.0518,  0.6406],\n        [-0.0278, -1.4302],\n        [-0.1689, -0.1168],\n        [ 0.9136, -0.6882],\n        [ 0.5192,  1.0466],\n        [ 0.5222,  1.1927]]) tensor([4.1721, 5.7795, 3.4662, 7.3638, 4.1241, 9.0224, 4.2584, 8.3639, 1.6802,\n        1.2006])\n"
    }
   ],
   "source": [
    "for X,y in data_iter:\n",
    "    print(X,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "LinearNet(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\n"
    }
   ],
   "source": [
    "#定义模型，使用pytorch预定义的层\n",
    "from torch import nn\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "    # forward 定义前向传播\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "net = LinearNet(num_inputs)\n",
    "print(net) # 使用print可以打印出网络的结构\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Sequential(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\nLinear(in_features=2, out_features=1, bias=True)\n"
    }
   ],
   "source": [
    "#也可以用nn.Sequential更方便地搭建网络，Sequantial是一个有序容器，网络层按照传入Sequantial的顺序依次添加\n",
    "net=nn.Sequential(nn.Linear(num_inputs,1))\n",
    "#或者：\n",
    "net=nn.Sequential()\n",
    "net.add_module('linear',nn.Linear(num_inputs,1))\n",
    "#net.add_module。。。\n",
    "#第三种写法：\n",
    "from collections import OrderedDict\n",
    "net=nn.Sequential(OrderedDict([('linear',nn.Linear(num_inputs,1))]))\n",
    "print(net)\n",
    "print(net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Parameter containing:\ntensor([[-0.6801, -0.5213]], requires_grad=True)\nParameter containing:\ntensor([-0.4130], requires_grad=True)\n"
    }
   ],
   "source": [
    "#通过net.parameters来查看模型所有可学习的参数\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([0.], requires_grad=True)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#注意torch.nn仅支持输入一个batch的样本，不支持单个样本的输入，如果只有单个样本，用input.unsqueeze(0)来添加一维\n",
    "\n",
    "#初始化模型参数，pytorch在init模块中提供了多种初始化方法，init是initializer的缩写形式。通过init.normal_把权重参数初始化为N(0,0.01^2)的正态，偏差初始化为0\n",
    "from torch.nn import init\n",
    "init.normal_(net[0].weight,mean=0,std=0.01)\n",
    "init.constant_(net[0].bias,val=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数，直接用nn模块提供的\n",
    "loss=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.03\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n"
    }
   ],
   "source": [
    "#定义优化算法，torch.optim提供常用优化算法\n",
    "import torch.optim as optim\n",
    "optimizer=optim.SGD(net.parameters(),lr=0.03)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'subnet1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-41ce10057e53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer =optim.SGD([\n\u001b[0;32m      3\u001b[0m                 \u001b[1;31m# 如果对某个参数不指定学习率，就使用最外层的默认学习率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                 \u001b[1;33m{\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubnet1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# lr=0.03\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                 \u001b[1;33m{\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubnet2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             ], lr=0.03)\n",
      "\u001b[1;32mD:\\anaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    537\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 539\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'subnet1'"
     ]
    }
   ],
   "source": [
    "#可以为不同子网络设置不同的学习率\n",
    "optimizer =optim.SGD([\n",
    "                # 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "                {'params': net.subnet1.parameters()}, # lr=0.03\n",
    "                {'params': net.subnet2.parameters(), 'lr': 0.01}\n",
    "            ], lr=0.03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调整学习率\n",
    "# 调整学习率\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] *= 0.1 # 学习率为之前的0.1倍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "epoch 1,loss: 0.000219\nepoch 2,loss: 0.000149\nepoch 3,loss: 0.000144\n"
    }
   ],
   "source": [
    "#训练模型\n",
    "num_epochs=3\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for X,y in data_iter:\n",
    "        output=net(X)\n",
    "        l=loss(output,y.view(-1,1))\n",
    "        optimizer.zero_grad()#梯度清零，等价于net.zero_grad\n",
    "        l.backward()\n",
    "        optimizer.step()#迭代模型参数，按小批量随机梯度下降\n",
    "    print('epoch %d,loss: %f'%(epoch,l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[2, -3.4] Parameter containing:\ntensor([[ 1.9998, -3.3998]], requires_grad=True)\n4.2 Parameter containing:\ntensor([4.2003], requires_grad=True)\n"
    }
   ],
   "source": [
    "dense=net[0]\n",
    "print(true_w,dense.weight)\n",
    "print(true_b,dense.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}